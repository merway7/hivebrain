[
  {
    "title": "GitHub Actions cache invalidation with cache-dependency-path",
    "category": "gotcha",
    "tags": ["github-actions", "caching", "ci"],
    "problem": "GitHub Actions cache hits are unreliable when dependency files change. Using only a static key causes stale caches to be restored after lockfile updates, leading to broken builds that pass CI but fail in production.",
    "solution": "Always include a hash of the lockfile in the cache key and use restore-keys as a fallback chain. Example:\n\n```yaml\n- uses: actions/cache@v4\n  with:\n    path: ~/.npm\n    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n    restore-keys: |\n      ${{ runner.os }}-node-\n```\n\nFor monorepos, scope the hash to the specific workspace:\n\n```yaml\n    key: ${{ runner.os }}-node-${{ hashFiles('apps/web/package-lock.json') }}\n```",
    "why": "The hash changes whenever the lockfile changes, forcing a cache miss and a fresh install. restore-keys allows partial hits on older caches so you still benefit from caching transitive deps that haven't changed.",
    "gotchas": [
      "cache@v4 has a 10 GB per repository limit; old caches are evicted automatically after 7 days of no access",
      "Cross-OS cache sharing does not work: always prefix the key with runner.os",
      "cache action does not fail the build on a miss, so a missing cache is silent"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["cache", "hashFiles", "restore-keys", "npm", "node_modules", "lockfile"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": "actions/cache@v4"
  },
  {
    "title": "Secrets in CI: never echo, always mask",
    "category": "principle",
    "tags": ["github-actions", "secrets", "security"],
    "problem": "Secrets accidentally get printed to CI logs via debug statements, error output, or shell expansion errors. Once printed, the secret is exposed in the build log even if the run is later deleted—logs may have already been scraped.",
    "solution": "Never use `echo $SECRET` in CI. Pass secrets via environment variables, never as positional CLI args. Use `::add-mask::` for dynamic values that aren't stored as GitHub secrets:\n\n```yaml\n- name: Mask derived token\n  run: echo \"::add-mask::${{ steps.get-token.outputs.token }}\"\n\n- name: Use secret safely\n  env:\n    API_KEY: ${{ secrets.API_KEY }}\n  run: curl -H \"Authorization: Bearer $API_KEY\" https://api.example.com\n```\n\nNever do: `run: curl -H \"Authorization: Bearer ${{ secrets.API_KEY }}\"`—this embeds the value in the workflow YAML before the shell sees it.",
    "why": "GitHub Actions redacts known secrets from logs, but only those registered as secrets. Inline `${{ secrets.X }}` in a run block gets interpolated into the shell command string, where a verbose command or errexit trace can print it.",
    "gotchas": [
      "Secrets are not passed to workflows triggered by forks—environment variables will be empty strings",
      "add-mask only applies to subsequent log lines in the same job, not previous ones",
      "set -x (xtrace) will print all env vars including secrets—never enable xtrace in production CI jobs"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["secrets", "mask", "security", "env vars", "add-mask", "token exposure"],
    "severity": "critical",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Matrix builds: fail-fast and include/exclude patterns",
    "category": "pattern",
    "tags": ["github-actions", "matrix", "ci"],
    "problem": "Matrix builds cancel all in-progress jobs when any single job fails (fail-fast: true is the default). This wastes useful signal from other matrix legs. Conversely, include/exclude misconfiguration silently drops matrix jobs.",
    "solution": "Disable fail-fast when you want to see all results. Use explicit include for additive combinations and exclude for subtractive ones:\n\n```yaml\njobs:\n  test:\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        node: [18, 20, 22]\n        exclude:\n          - os: windows-latest\n            node: 18\n        include:\n          - os: ubuntu-latest\n            node: 22\n            experimental: true\n    runs-on: ${{ matrix.os }}\n    continue-on-error: ${{ matrix.experimental == true }}\n```",
    "why": "fail-fast: false surfaces failures across all combinations. continue-on-error on experimental legs lets the overall job succeed even if the experimental leg fails, so you get data without blocking merges.",
    "gotchas": [
      "include entries that don't match any existing matrix combination create new standalone jobs—useful but surprising",
      "Matrix values are always strings at the YAML level; compare carefully in conditionals",
      "Maximum of 256 jobs per matrix"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["matrix", "fail-fast", "include", "exclude", "strategy", "cross-platform"],
    "severity": "moderate",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Artifact retention: set explicit retention-days to control storage costs",
    "category": "pattern",
    "tags": ["github-actions", "artifacts", "storage"],
    "problem": "GitHub Actions artifacts default to 90-day retention. Large build outputs (binaries, coverage reports, test results) accumulate silently and consume storage quota, eventually triggering billing surprises or hard limits.",
    "solution": "Always set retention-days explicitly based on artifact purpose:\n\n```yaml\n- uses: actions/upload-artifact@v4\n  with:\n    name: build-output-${{ github.run_id }}\n    path: dist/\n    retention-days: 7   # short-lived build artifacts\n\n- uses: actions/upload-artifact@v4\n  with:\n    name: release-${{ github.ref_name }}\n    path: release/*.tar.gz\n    retention-days: 90  # release assets kept longer\n```\n\nFor download in the same workflow:\n\n```yaml\n- uses: actions/download-artifact@v4\n  with:\n    name: build-output-${{ github.run_id }}\n    path: dist/\n```",
    "why": "Short-lived CI artifacts (coverage, logs, bundles) don't need 90 days. Setting 1-7 days for transient artifacts dramatically reduces storage consumption.",
    "gotchas": [
      "Artifacts from the same run can only be downloaded by jobs in that same run unless you use actions/download-artifact with run-id",
      "artifact names must be unique per run; duplicate names cause the upload to fail in v4",
      "retention-days cannot exceed the repository or organisation policy"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["artifact", "retention", "storage", "upload-artifact", "download-artifact", "cost"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": "actions/upload-artifact@v4, actions/download-artifact@v4"
  },
  {
    "title": "Self-hosted runners: ephemeral runners prevent environment pollution",
    "category": "principle",
    "tags": ["github-actions", "self-hosted-runners", "infrastructure"],
    "problem": "Persistent self-hosted runners accumulate state between jobs: leftover Docker containers, npm caches from other repos, modified system files, or leaked secrets in environment variables. This causes non-reproducible builds and security risks.",
    "solution": "Use ephemeral (--ephemeral) self-hosted runners that terminate after one job. Pair with an autoscaler (Actions Runner Controller on Kubernetes, or EC2 via philips-labs/terraform-aws-github-runner):\n\n```bash\n# Register ephemeral runner\n./config.sh \\\n  --url https://github.com/ORG/REPO \\\n  --token $RUNNER_TOKEN \\\n  --ephemeral \\\n  --unattended\n./run.sh\n```\n\nIn the workflow, target self-hosted runners by label:\n\n```yaml\njobs:\n  build:\n    runs-on: [self-hosted, linux, x64, ephemeral]\n```",
    "why": "Ephemeral runners start fresh for every job. No state leaks between jobs, no dirty workspace, and compromised runners auto-rotate after use, limiting blast radius of a supply chain attack.",
    "gotchas": [
      "Ephemeral runners deregister after one job—you cannot re-run a failed job on the same runner instance",
      "Docker-in-Docker on ephemeral runners needs --privileged or a socket mount; plan your runner image carefully",
      "Runner registration tokens expire after 1 hour—automate token refresh in your provisioning scripts"
    ],
    "language": "bash",
    "framework": null,
    "environment": ["linux", "kubernetes", "aws"],
    "error_messages": [],
    "keywords": ["self-hosted", "ephemeral", "runner", "ARC", "autoscaler", "security", "isolation"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Docker layer caching in GitHub Actions with cache-from and cache-to",
    "category": "pattern",
    "tags": ["github-actions", "docker", "caching"],
    "problem": "Every CI run rebuilds Docker images from scratch, wasting 3-10 minutes pulling base images and reinstalling dependencies. Without layer caching, even a one-line change triggers a full rebuild.",
    "solution": "Use BuildKit's inline cache or registry cache with docker/build-push-action:\n\n```yaml\n- uses: docker/setup-buildx-action@v3\n\n- uses: docker/build-push-action@v5\n  with:\n    context: .\n    push: false\n    tags: myapp:latest\n    cache-from: type=gha\n    cache-to: type=gha,mode=max\n```\n\nFor registry-based caching (better for self-hosted runners):\n\n```yaml\n- uses: docker/build-push-action@v5\n  with:\n    cache-from: type=registry,ref=ghcr.io/org/myapp:cache\n    cache-to: type=registry,ref=ghcr.io/org/myapp:cache,mode=max\n```",
    "why": "BuildKit stores individual layer blobs. mode=max caches intermediate layers too, not just the final image. The gha backend uses GitHub's cache API; registry backend survives runner restarts and works across forks.",
    "gotchas": [
      "type=gha shares the Actions cache 10 GB quota with other caches in the repo",
      "mode=max increases cache size significantly—monitor storage usage",
      "Registry cache requires authentication before the build step",
      "DOCKER_BUILDKIT=1 is required for non-Buildx builds; docker/setup-buildx-action handles this automatically"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["docker", "layer cache", "buildkit", "build-push-action", "gha", "registry cache"],
    "severity": "moderate",
    "context": null,
    "code_snippets": [],
    "version_info": "docker/build-push-action@v5, docker/setup-buildx-action@v3"
  },
  {
    "title": "Environment protection rules block accidental production deploys",
    "category": "pattern",
    "tags": ["github-actions", "environments", "deployment", "security"],
    "problem": "Without environment protection rules, any branch can trigger a production deployment. A dev pushing to main directly, or a malicious PR, can deploy broken code to production.",
    "solution": "Configure environment protection rules in GitHub Settings > Environments:\n\n1. Add required reviewers (at least one person must approve before deployment proceeds)\n2. Restrict deployments to protected branches only (main, release/*)\n3. Set a wait timer if you want a cooldown window\n\nIn the workflow:\n\n```yaml\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://myapp.com\n    steps:\n      - name: Deploy\n        run: ./deploy.sh\n```\n\nGitHub will pause the job and send review requests before running any steps.",
    "why": "Environment protection rules are enforced server-side by GitHub, not in the workflow YAML. Even if someone modifies the workflow, the protection rules still apply. This is the correct place to gate production access.",
    "gotchas": [
      "Environments are only available on public repos or with GitHub Team/Enterprise for private repos",
      "Required reviewers cannot approve their own deployment",
      "The 'url' field in environment: populates the deployment link in the GitHub UI—always set it"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["environment", "protection rules", "reviewers", "production", "deployment gate", "branch restriction"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Reusable workflows with workflow_call for DRY CI pipelines",
    "category": "pattern",
    "tags": ["github-actions", "reusable-workflows", "DRY"],
    "problem": "Large repositories duplicate the same CI steps (lint, test, build) across dozens of workflow files. When you need to update the Node version or add a new step, you must edit every file individually.",
    "solution": "Extract shared logic into a reusable workflow triggered by workflow_call:\n\n```yaml\n# .github/workflows/_reusable-test.yml\non:\n  workflow_call:\n    inputs:\n      node-version:\n        required: true\n        type: string\n    secrets:\n      NPM_TOKEN:\n        required: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ inputs.node-version }}\n      - run: npm ci\n        env:\n          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n      - run: npm test\n```\n\nCaller workflow:\n\n```yaml\n# .github/workflows/ci.yml\njobs:\n  test:\n    uses: ./.github/workflows/_reusable-test.yml\n    with:\n      node-version: '20'\n    secrets:\n      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n```",
    "why": "One place to update CI logic, consistent behaviour across all callers, and secrets are explicitly passed so callers must opt-in rather than inheriting everything.",
    "gotchas": [
      "Reusable workflows count toward the 20 job limit per workflow",
      "Secrets must be explicitly passed—callers cannot pass 'inherit' unless using secrets: inherit syntax",
      "You cannot call a reusable workflow from within a reusable workflow more than 3 levels deep",
      "Inputs are strings, numbers, or booleans only—no arrays or objects"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["reusable workflow", "workflow_call", "DRY", "inputs", "secrets", "shared CI"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Conditional steps with if expressions and job outputs",
    "category": "pattern",
    "tags": ["github-actions", "conditional", "if", "outputs"],
    "problem": "CI pipelines run expensive steps (e2e tests, deployments) even when they are not needed, for example when only documentation files change, or when the build step failed.",
    "solution": "Use if conditions on steps and jobs. Read changed files or previous step results:\n\n```yaml\njobs:\n  changes:\n    runs-on: ubuntu-latest\n    outputs:\n      src: ${{ steps.filter.outputs.src }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v3\n        id: filter\n        with:\n          filters: |\n            src:\n              - 'src/**'\n              - 'package*.json'\n\n  test:\n    needs: changes\n    if: needs.changes.outputs.src == 'true'\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm test\n\n  deploy:\n    needs: [test]\n    if: success() && github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - run: ./deploy.sh\n```",
    "why": "Skipping jobs based on file changes saves runner minutes. Using success() and explicit branch checks prevents accidental deploys from PR branches or after test failures.",
    "gotchas": [
      "if: always() bypasses both success and failure checks—use only when you genuinely want a step to run regardless",
      "Skipped jobs are still shown in the UI as skipped, not hidden",
      "Job outputs from skipped jobs are empty strings, not null—guard comparisons accordingly"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["if condition", "conditional step", "paths-filter", "outputs", "success()", "skip"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": "dorny/paths-filter@v3"
  },
  {
    "title": "Job and step timeouts to prevent runaway CI bills",
    "category": "principle",
    "tags": ["github-actions", "timeout", "cost"],
    "problem": "Hung processes (waiting for user input, deadlocked test, infinite loop in a script) cause CI jobs to run for the full 6-hour default timeout, consuming all runner minutes and blocking the queue.",
    "solution": "Set timeout-minutes on both jobs and individual steps:\n\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    timeout-minutes: 15  # entire job\n    steps:\n      - name: Install dependencies\n        run: npm ci\n        timeout-minutes: 5  # this step only\n\n      - name: Run tests\n        run: npm test\n        timeout-minutes: 10\n```\n\nFor long-running processes, add explicit timeouts inside the command:\n\n```bash\ntimeout 300 ./run-integration-tests.sh\n```",
    "why": "A 15-minute timeout on a job that normally takes 5 minutes gives headroom for slow days while preventing the 6-hour worst case. Step-level timeouts pinpoint which step is hanging.",
    "gotchas": [
      "timeout-minutes is in whole minutes only—you cannot set 90 seconds",
      "When a timeout fires, the job is cancelled without running post steps unless you use if: always() on cleanup steps",
      "Self-hosted runners have no timeout by default; set one explicitly"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["timeout", "timeout-minutes", "runaway", "hung process", "cost control", "CI limit"],
    "severity": "moderate",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Retry failed steps with a retry action or shell loop",
    "category": "pattern",
    "tags": ["github-actions", "retry", "flaky-tests", "resilience"],
    "problem": "Network-dependent steps (npm install, Docker pull, API calls) fail transiently in CI, causing the entire pipeline to fail even though a retry would succeed. Rebuilding manually wastes time.",
    "solution": "Use nick-fields/retry for step-level retries, or implement a shell retry loop:\n\n```yaml\n- uses: nick-fields/retry@v3\n  with:\n    timeout_minutes: 5\n    max_attempts: 3\n    retry_wait_seconds: 10\n    command: npm ci\n```\n\nOr a simple shell approach:\n\n```yaml\n- name: Install with retry\n  run: |\n    for i in 1 2 3; do\n      npm ci && break\n      echo \"Attempt $i failed, retrying in 10s...\"\n      sleep 10\n    done\n```\n\nFor flaky tests, consider pytest-rerunfailures or jest --testRepeats rather than retrying the whole step.",
    "why": "Transient failures are a network reality in CI. Retrying at the step level avoids wasting the entire job run. Shell loops are dependency-free and easy to audit.",
    "gotchas": [
      "Retrying tests that have side effects (database mutations, file writes) can leave state that causes the retry itself to fail differently",
      "Excessive retries mask real flakiness—track flaky test rates separately",
      "nick-fields/retry does not retry on specific exit codes by default; configure retry_on if needed"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["retry", "flaky", "transient failure", "npm ci", "resilience", "network error"],
    "severity": "moderate",
    "context": null,
    "code_snippets": [],
    "version_info": "nick-fields/retry@v3"
  },
  {
    "title": "Concurrency groups to cancel stale PR runs",
    "category": "pattern",
    "tags": ["github-actions", "concurrency", "pull-request", "cost"],
    "problem": "Every push to a PR branch triggers a new CI run. Old runs keep executing even after new commits invalidate them, wasting runner minutes and cluttering the PR status checks UI.",
    "solution": "Use concurrency groups with cancel-in-progress:\n\n```yaml\nconcurrency:\n  group: ci-${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n```\n\nFor deployment workflows, never cancel in-progress production deploys—use a different group strategy:\n\n```yaml\nconcurrency:\n  group: deploy-production\n  cancel-in-progress: false  # queue instead of cancel\n```\n\nFor PR comment-triggered workflows, use the PR number:\n\n```yaml\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}\n  cancel-in-progress: true\n```",
    "why": "cancel-in-progress: true immediately kills the old run when a new one starts for the same group. For production, false means the new run waits, preventing race conditions between two deployments.",
    "gotchas": [
      "Cancelled runs are marked as cancelled, not failed—they do not block merges if required checks are configured",
      "The group string must be unique enough to not collide across unrelated workflows",
      "cancel-in-progress: true on a deploy job can leave infrastructure in a half-deployed state"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["concurrency", "cancel-in-progress", "PR run", "stale", "queue", "cost savings"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Blue-green deployments with traffic switching",
    "category": "pattern",
    "tags": ["deployment", "blue-green", "zero-downtime", "infrastructure"],
    "problem": "In-place deployments cause downtime: the old version is stopped before the new version is ready, or a bad deploy corrupts the running environment with no fast rollback path.",
    "solution": "Run two identical environments (blue and green). Deploy to the inactive one, run smoke tests, then switch traffic:\n\n```yaml\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - name: Determine inactive slot\n        id: slot\n        run: |\n          ACTIVE=$(aws elbv2 describe-tags --resource-arns $TG_ARN \\\n            --query 'TagDescriptions[0].Tags[?Key==`Slot`].Value' --output text)\n          echo \"inactive=$([[ $ACTIVE == 'blue' ]] && echo green || echo blue)\" >> $GITHUB_OUTPUT\n\n      - name: Deploy to inactive slot\n        run: |\n          ./deploy.sh ${{ steps.slot.outputs.inactive }}\n\n      - name: Run smoke tests\n        run: ./smoke-test.sh ${{ steps.slot.outputs.inactive }}\n\n      - name: Switch traffic\n        run: |\n          aws elbv2 modify-listener --listener-arn $LISTENER_ARN \\\n            --default-actions Type=forward,TargetGroupArn=${{ steps.slot.outputs.inactive == 'blue' && env.BLUE_TG || env.GREEN_TG }}\n```",
    "why": "The old environment stays live until the new one is verified. Rollback is instant—just switch traffic back. No data migration or re-deploy needed.",
    "gotchas": [
      "Database migrations must be backward-compatible since both environments share the same DB during the cutover window",
      "Sessions pinned to the old environment will be dropped at traffic switch—plan for graceful session handling",
      "Infrastructure costs are doubled during the deployment window"
    ],
    "language": "bash",
    "framework": null,
    "environment": ["aws", "linux"],
    "error_messages": [],
    "keywords": ["blue-green", "zero downtime", "traffic switch", "rollback", "slot", "load balancer"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Canary releases with progressive traffic shifting",
    "category": "pattern",
    "tags": ["deployment", "canary", "progressive-delivery", "monitoring"],
    "problem": "Deploying directly to 100% of traffic means a bad release hits all users immediately. By the time monitoring alerts fire, thousands of users have already seen the error.",
    "solution": "Route a small percentage of traffic to the new version and increase gradually based on error rate:\n\n```yaml\njobs:\n  canary-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy canary (5% traffic)\n        run: |\n          kubectl apply -f manifests/canary.yaml\n          kubectl patch virtualservice myapp --type merge -p \\\n            '{\"spec\":{\"http\":[{\"route\":[{\"destination\":{\"subset\":\"v2\"},\"weight\":5},{\"destination\":{\"subset\":\"v1\"},\"weight\":95}]}]}}'\n\n      - name: Monitor error rate for 5 minutes\n        run: |\n          sleep 300\n          ERROR_RATE=$(./get-error-rate.sh canary)\n          if (( $(echo \"$ERROR_RATE > 1.0\" | bc -l) )); then\n            echo \"Error rate $ERROR_RATE% too high, rolling back\"\n            ./rollback-canary.sh\n            exit 1\n          fi\n\n      - name: Promote to 100%\n        run: ./promote-canary.sh\n```",
    "why": "5% canary limits blast radius to 5% of users. If error rates spike, rollback is immediate. Automated monitoring removes the need for a human to watch dashboards during every deploy.",
    "gotchas": [
      "Canary and stable versions must be compatible with the same database schema simultaneously",
      "Stateful services (WebSockets, gRPC streams) should not use canary without session affinity consideration",
      "Monitor p99 latency as well as error rate—a slow canary is also a bad canary"
    ],
    "language": "bash",
    "framework": null,
    "environment": ["kubernetes", "linux"],
    "error_messages": [],
    "keywords": ["canary", "progressive delivery", "traffic shift", "error rate", "rollback", "istio"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Feature flags for safe deployments decoupled from releases",
    "category": "principle",
    "tags": ["feature-flags", "deployment", "continuous-delivery"],
    "problem": "Features must be fully tested before release, which forces long-lived feature branches that diverge from main and cause painful merge conflicts. Deploying and releasing are coupled, preventing incremental delivery.",
    "solution": "Merge code to main behind a feature flag—deploy it disabled, then enable it gradually:\n\n```typescript\n// Using LaunchDarkly / Unleash / custom flag store\nconst flags = await flagClient.getFlags(userId);\n\nif (flags['new-checkout-flow']) {\n  return renderNewCheckout();\n} else {\n  return renderLegacyCheckout();\n}\n```\n\nIn CI, test both code paths:\n\n```yaml\n- name: Test with flag enabled\n  env:\n    FEATURE_NEW_CHECKOUT: 'true'\n  run: npm test\n\n- name: Test with flag disabled\n  env:\n    FEATURE_NEW_CHECKOUT: 'false'\n  run: npm test\n```\n\nFlag rollout: 1% -> 10% -> 50% -> 100%, with automatic rollback if error rate exceeds threshold.",
    "why": "Decoupling deploy from release means code ships continuously without waiting for a release window. Rollback is a flag flip, not a code revert and redeploy.",
    "gotchas": [
      "Flag debt accumulates fast—set a removal date for every flag at creation time",
      "Testing all flag combinations becomes exponential; test only the critical paths for each flag",
      "Flag evaluation adds latency; cache flag values with a short TTL rather than fetching per-request"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["feature flags", "feature toggle", "continuous delivery", "rollout", "trunk-based development"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Terraform state locking prevents concurrent apply race conditions",
    "category": "gotcha",
    "tags": ["terraform", "state", "infrastructure-as-code"],
    "problem": "Two terraform apply runs executing simultaneously against the same state file corrupt the state. One apply reads stale state, both write conflicting changes, and infrastructure drifts from the declared configuration.",
    "solution": "Use a backend that supports state locking. For S3 + DynamoDB:\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-tfstate-bucket\"\n    key            = \"prod/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"\n    encrypt        = true\n  }\n}\n```\n\nDynamoDB table must have LockID as the partition key (String type).\n\nIn CI, use terraform plan + apply in sequence with the -lock-timeout flag:\n\n```yaml\n- run: terraform plan -lock-timeout=10m -out=tfplan\n- run: terraform apply -lock-timeout=10m tfplan\n```",
    "why": "DynamoDB provides conditional writes with atomic lock acquisition. If a lock exists, the second apply waits or fails rather than proceeding with stale state.",
    "gotchas": [
      "If a CI job is cancelled mid-apply, the lock may remain in DynamoDB; use terraform force-unlock with the lock ID to recover",
      "Terraform Cloud and HCP Terraform provide state locking automatically without DynamoDB",
      "Never use local state in CI—it is lost when the runner terminates"
    ],
    "language": "bash",
    "framework": null,
    "environment": ["aws"],
    "error_messages": [
      "Error acquiring the state lock",
      "Lock Info: ID:"
    ],
    "keywords": ["terraform", "state lock", "DynamoDB", "S3 backend", "race condition", "concurrent apply"],
    "severity": "critical",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Monitoring alerts: alert on symptoms not causes for actionable on-call",
    "category": "principle",
    "tags": ["monitoring", "alerting", "observability", "SRE"],
    "problem": "Alerts fire on internal metrics (CPU > 80%, memory > 70%, queue depth > 1000) that do not directly indicate user impact. On-call engineers get paged at 3am for CPU spikes that caused no visible degradation.",
    "solution": "Alert on user-facing symptoms: error rate, latency p99, and availability:\n\n```yaml\n# Prometheus alerting rule example\ngroups:\n  - name: user-facing\n    rules:\n      - alert: HighErrorRate\n        expr: |\n          sum(rate(http_requests_total{status=~\"5..\"}[5m]))\n          /\n          sum(rate(http_requests_total[5m])) > 0.01\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Error rate above 1% for 2 minutes\"\n\n      - alert: HighLatency\n        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 2\n        for: 5m\n        labels:\n          severity: warning\n```\n\nInternal metrics (CPU, memory) become dashboards for diagnosis, not pages.",
    "why": "A user cannot feel your CPU usage. They can feel a 5xx response or a 10-second page load. Symptom-based alerts directly correlate to SLA violations and customer impact.",
    "gotchas": [
      "The 'for' duration prevents false positives from brief spikes—always use at least 2m for critical alerts",
      "Alert fatigue from too many firing alerts causes engineers to start ignoring all alerts—be conservative with thresholds",
      "Dead man's switch alert: alert if no data is received—a silent system can mean monitoring is broken, not that things are fine"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["monitoring", "alerting", "error rate", "latency", "p99", "prometheus", "SLO"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Vercel deployment with preview URLs and production promotion",
    "category": "pattern",
    "tags": ["vercel", "deployment", "preview", "ci"],
    "problem": "Vercel deployments triggered from GitHub require the Vercel GitHub integration, which gives limited control over when and how deployments happen. Teams need to run custom pre-deployment steps or gate production on CI results.",
    "solution": "Use the Vercel CLI in GitHub Actions for full control:\n\n```yaml\njobs:\n  deploy-preview:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm ci && npm run build\n      - name: Deploy preview\n        id: deploy\n        run: |\n          URL=$(npx vercel deploy --token=${{ secrets.VERCEL_TOKEN }})\n          echo \"url=$URL\" >> $GITHUB_OUTPUT\n      - uses: actions/github-script@v7\n        with:\n          script: |\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: `Preview: ${{ steps.deploy.outputs.url }}`\n            })\n\n  deploy-production:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    environment: production\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm ci && npm run build\n      - run: npx vercel deploy --prod --token=${{ secrets.VERCEL_TOKEN }}\n```",
    "why": "CLI-driven deployment lets you run tests before deploying, post preview URLs as PR comments, and require environment approval before promoting to production—none of which the automatic GitHub integration supports natively.",
    "gotchas": [
      "VERCEL_TOKEN must be a personal token or team token with deployment permissions; project-scoped tokens cannot promote to production",
      "vercel deploy without --prod creates a preview URL—always verify you have the right flag",
      "Build step must run before vercel deploy if the output directory is not the repo root"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["vercel", "preview URL", "CLI deploy", "production promotion", "PR comment", "vercel token"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Netlify configuration via netlify.toml for reproducible deployments",
    "category": "pattern",
    "tags": ["netlify", "deployment", "configuration"],
    "problem": "Netlify site settings configured in the UI are invisible to code review and can be accidentally changed by any team member. Build commands, publish directories, and redirect rules diverge between dev and production.",
    "solution": "Commit a netlify.toml at the repository root:\n\n```toml\n[build]\n  command = \"npm run build\"\n  publish = \"dist\"\n  environment = { NODE_VERSION = \"20\" }\n\n[build.environment]\n  NEXT_TELEMETRY_DISABLED = \"1\"\n\n[[redirects]]\n  from = \"/api/*\"\n  to = \"https://api.myapp.com/:splat\"\n  status = 200\n  force = true\n\n[[redirects]]\n  from = \"/*\"\n  to = \"/index.html\"\n  status = 200\n\n[context.deploy-preview]\n  command = \"npm run build:preview\"\n\n[context.branch-deploy]\n  command = \"npm run build\"\n```\n\nRedirect rules in netlify.toml take precedence over UI-configured rules.",
    "why": "Infrastructure as code: the build configuration lives in the repository, is code-reviewed, versioned, and applied consistently to every deploy. No surprise UI changes affect production.",
    "gotchas": [
      "netlify.toml redirect rules are evaluated top-to-bottom; the SPA catch-all must be last",
      "Environment variables set in netlify.toml are visible in build logs—do not put secrets there, use the Netlify UI or CLI for secrets",
      "NODE_VERSION in build.environment controls the Node version; not setting it uses Netlify's default which may not match your development environment"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["netlify", "netlify.toml", "redirects", "build config", "SPA", "environment"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Composite actions for reusable multi-step logic within a repo",
    "category": "pattern",
    "tags": ["github-actions", "composite-actions", "reusable"],
    "problem": "The same sequence of 4-5 steps appears in multiple jobs within a repository (e.g., checkout + setup Node + restore cache + install). Reusable workflows add an extra job layer and have input type limitations.",
    "solution": "Create a composite action in .github/actions/setup-node:\n\n```yaml\n# .github/actions/setup-node/action.yml\nname: Setup Node\ndescription: Checkout, setup Node, restore cache, and install dependencies\ninputs:\n  node-version:\n    description: Node.js version\n    required: false\n    default: '20'\nruns:\n  using: composite\n  steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-node@v4\n      with:\n        node-version: ${{ inputs.node-version }}\n    - uses: actions/cache@v4\n      with:\n        path: node_modules\n        key: ${{ runner.os }}-nm-${{ hashFiles('package-lock.json') }}\n    - run: npm ci\n      shell: bash\n```\n\nUse it in any job:\n\n```yaml\nsteps:\n  - uses: ./.github/actions/setup-node\n    with:\n      node-version: '22'\n```",
    "why": "Composite actions run in the same job, share the same workspace and environment, and do not add extra job overhead. They are simpler than reusable workflows for step-level reuse.",
    "gotchas": [
      "Every run step in a composite action must specify shell explicitly",
      "Composite actions cannot use continue-on-error at the action level—only individual steps",
      "Secrets cannot be inputs to composite actions; pass them as environment variables from the caller"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["composite action", "reusable steps", "action.yml", "DRY", "local action", "shell"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Dependabot configuration for automated dependency updates",
    "category": "pattern",
    "tags": ["dependabot", "security", "dependency-management", "github"],
    "problem": "Outdated dependencies accumulate silently. Security vulnerabilities in transitive dependencies go unnoticed for months. Manual dependency updates are tedious and skipped under deadline pressure.",
    "solution": "Configure .github/dependabot.yml:\n\n```yaml\nversion: 2\nupdates:\n  - package-ecosystem: npm\n    directory: /\n    schedule:\n      interval: weekly\n      day: monday\n      time: '09:00'\n      timezone: Europe/Paris\n    open-pull-requests-limit: 10\n    groups:\n      dev-dependencies:\n        dependency-type: development\n      aws-sdk:\n        patterns:\n          - '@aws-sdk/*'\n    ignore:\n      - dependency-name: some-legacy-package\n        versions: ['> 1.x']\n\n  - package-ecosystem: github-actions\n    directory: /\n    schedule:\n      interval: weekly\n    groups:\n      actions:\n        patterns:\n          - '*'\n```",
    "why": "Grouping related packages (all @aws-sdk/* into one PR) reduces PR noise. Scheduling on Monday morning means updates arrive at the start of the week when engineers can review them.",
    "gotchas": [
      "Dependabot PRs for major version bumps require manual review—they will not auto-merge if you only allow patch auto-merges",
      "open-pull-requests-limit default is 5; if you have many packages this fills up and stops new PRs from being created",
      "Dependabot does not update packages that are explicitly ignored in package.json resolutions field"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["dependabot", "dependency update", "security", "grouping", "auto-merge", "schedule"],
    "severity": "moderate",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "release-please for automated changelog and release PR generation",
    "category": "pattern",
    "tags": ["release-please", "changelog", "semantic-versioning", "github"],
    "problem": "Manually maintaining CHANGELOGs and bumping version numbers is error-prone and forgotten under pressure. Releases go out without a changelog, or with incorrect version numbers that violate semantic versioning.",
    "solution": "Configure release-please to automate releases from conventional commits:\n\n```yaml\n# .github/workflows/release.yml\non:\n  push:\n    branches: [main]\n\njobs:\n  release-please:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: googleapis/release-please-action@v4\n        id: release\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          release-type: node\n\n      - uses: actions/checkout@v4\n        if: ${{ steps.release.outputs.release_created }}\n\n      - name: Publish to npm\n        if: ${{ steps.release.outputs.release_created }}\n        run: npm publish\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n```\n\nrelease-please.json for monorepo:\n\n```json\n{\n  \"packages\": {\n    \"packages/api\": { \"release-type\": \"node\" },\n    \"packages/web\": { \"release-type\": \"node\" }\n  }\n}\n```",
    "why": "release-please reads conventional commit messages (feat:, fix:, feat!:) to determine version bumps and generates the CHANGELOG automatically. The release PR is opened automatically and merged when you are ready to release.",
    "gotchas": [
      "Commits must follow conventional commits spec exactly—typos like 'Feat:' (capital F) are ignored",
      "release-please opens one PR at a time and updates it as new commits land—merging that PR triggers the release",
      "The GITHUB_TOKEN needs write permissions to contents and pull-requests"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["release-please", "changelog", "conventional commits", "semantic versioning", "npm publish", "monorepo release"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": "googleapis/release-please-action@v4"
  },
  {
    "title": "Semantic versioning enforcement with commitlint in CI",
    "category": "pattern",
    "tags": ["semantic-versioning", "commitlint", "conventional-commits", "ci"],
    "problem": "Teams adopt conventional commits informally but inconsistently. Commits like 'fix stuff' or 'WIP' pollute the history, break changelog generation, and make it impossible to automate version bumps.",
    "solution": "Enforce conventional commits with commitlint in CI:\n\n```yaml\n# .github/workflows/lint-commits.yml\non:\n  pull_request:\n    types: [opened, synchronize, reopened, edited]\n\njobs:\n  commitlint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n      - run: npm install --no-save @commitlint/cli @commitlint/config-conventional\n      - run: |\n          npx commitlint \\\n            --from ${{ github.event.pull_request.base.sha }} \\\n            --to ${{ github.event.pull_request.head.sha }} \\\n            --verbose\n```\n\ncommitlint.config.js:\n\n```js\nmodule.exports = { extends: ['@commitlint/config-conventional'] };\n```",
    "why": "Blocking the PR until all commits pass commitlint enforces the convention at the point of code review, not after the fact. fetch-depth: 0 is required so commitlint can access all commits in the PR range.",
    "gotchas": [
      "Squash-merge PRs only need the PR title to follow convention—individual commit messages don't matter if you squash",
      "fetch-depth: 0 fetches the entire repo history which is slow on large repos; use fetch-depth with a computed depth instead",
      "Breaking changes must use feat!: or include 'BREAKING CHANGE:' in the footer—not in the subject line"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [
      "⧗   input: fix stuff",
      "✖   subject may not be empty [subject-empty]"
    ],
    "keywords": ["commitlint", "conventional commits", "semantic versioning", "PR lint", "fetch-depth", "breaking change"],
    "severity": "moderate",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Monorepo CI with path-based job filtering",
    "category": "pattern",
    "tags": ["monorepo", "ci", "github-actions", "turborepo"],
    "problem": "In a monorepo, every CI run builds and tests all packages even when only one package changed. As the repo grows, CI time scales with the number of packages rather than the number of changes.",
    "solution": "Use Turborepo's --filter with git diff to run only affected packages:\n\n```yaml\njobs:\n  ci:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # needed for turbo to compute affected packages\n\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - run: npm ci\n\n      - name: Run affected tests\n        run: |\n          npx turbo run test build lint \\\n            --filter='...[origin/main]' \\\n            --cache-dir=.turbo\n\n      - uses: actions/cache@v4\n        with:\n          path: .turbo\n          key: turbo-${{ github.sha }}\n          restore-keys: turbo-\n```\n\nFor nx monorepos: `npx nx affected --target=test --base=origin/main`",
    "why": "--filter='...[origin/main]' tells Turbo to run only packages that have changed relative to main, plus their dependents. This scales CI time to the size of the change, not the size of the repo.",
    "gotchas": [
      "fetch-depth: 0 is required; without it git cannot compute the diff and Turbo falls back to running everything",
      "Turbo remote cache (Vercel or self-hosted) shares cache across CI runs—hit rates improve dramatically after the first run",
      "Packages that share a dependency get rebuilt when that dependency changes, even if their own code did not change"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["monorepo", "turborepo", "nx", "affected packages", "CI filter", "fetch-depth"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Branch protection rules: require status checks before merging",
    "category": "principle",
    "tags": ["github", "branch-protection", "status-checks", "code-quality"],
    "problem": "Developers merge PRs before CI has finished, or bypass failing checks using admin override. Broken code lands on main, blocking the entire team and requiring a revert or hotfix.",
    "solution": "Configure branch protection rules in GitHub Settings > Branches for the main branch:\n\n1. Require a pull request before merging\n2. Require status checks to pass before merging (check 'Require branches to be up to date before merging')\n3. Add specific required status checks: 'test (ubuntu-latest, 20)', 'lint', 'typecheck'\n4. Do not allow bypassing the above settings (prevents admins from force-merging)\n5. Require linear history (prevents merge commits, enforces rebase/squash)\n\nIn GitHub rulesets (newer API), use:\n\n```json\n{\n  \"required_status_checks\": {\n    \"strict\": true,\n    \"checks\": [\n      { \"context\": \"test\" },\n      { \"context\": \"lint\" }\n    ]\n  }\n}\n```",
    "why": "Status checks are the primary gate preventing broken code from landing on main. 'Require branches to be up to date' prevents the race condition where two PRs both pass CI independently but fail when merged together.",
    "gotchas": [
      "Required status check names must match exactly—if the check is named 'test (ubuntu-latest, 20)' the matrix job name must match",
      "If a required check never runs (e.g., it is skipped via if:), the branch cannot be merged—use a separate always-passing job as a sentinel",
      "Admins bypassing checks is a common foot-gun; 'Do not allow bypassing' is a separate setting from 'Restrict who can push'"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["branch protection", "status checks", "required checks", "linear history", "merge gate", "admin bypass"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "Rollback strategy: automated revert on deploy failure",
    "category": "pattern",
    "tags": ["deployment", "rollback", "automation", "reliability"],
    "problem": "When a deploy fails, engineers manually identify the last good commit, run a revert, wait for CI, and re-deploy. This takes 15-30 minutes during which users are affected. There is no documented runbook so every incident is improvised.",
    "solution": "Build automated rollback into the deployment workflow:\n\n```yaml\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Record current deployed SHA\n        id: current\n        run: echo \"sha=$(./get-current-deployed-sha.sh)\" >> $GITHUB_OUTPUT\n\n      - name: Deploy\n        id: deploy\n        run: ./deploy.sh ${{ github.sha }}\n\n      - name: Smoke test\n        id: smoke\n        run: ./smoke-test.sh\n\n      - name: Rollback on failure\n        if: failure() && steps.deploy.outcome == 'success'\n        run: |\n          echo \"Smoke test failed, rolling back to ${{ steps.current.outputs.sha }}\"\n          ./deploy.sh ${{ steps.current.outputs.sha }}\n          ./notify-slack.sh \"Production rolled back to ${{ steps.current.outputs.sha }}\"\n```",
    "why": "Automated rollback reduces MTTR (mean time to recovery) from 30 minutes to under 5. The condition 'steps.deploy.outcome == success' ensures rollback only runs if the deploy succeeded but smoke tests failed—not if the deploy itself failed midway.",
    "gotchas": [
      "Rolling back infrastructure changes (database migrations, schema changes) is much harder than rolling back application code—plan separately",
      "The rollback step itself can fail; add monitoring on the rollback step and alert if it fails",
      "Rollback to the previous SHA does not account for environment changes (env vars, secrets) that may have been updated alongside the deploy"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["rollback", "revert", "smoke test", "MTTR", "automated recovery", "deploy failure"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "GitHub Environments: deployment history and audit trail",
    "category": "pattern",
    "tags": ["github-actions", "environments", "audit", "deployment-history"],
    "problem": "When something breaks in production, it is difficult to know what was deployed when, by whom, and from which commit. The audit trail is scattered across Slack, git log, and CI run history.",
    "solution": "Use GitHub Environments to get automatic deployment history in the GitHub UI:\n\n```yaml\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://myapp.com\n    steps:\n      - run: ./deploy.sh\n```\n\nEvery run creates a deployment record at github.com/org/repo/deployments showing:\n- Which commit was deployed\n- Who triggered it\n- When it started and ended\n- Whether it succeeded\n- A direct link to the deployed URL\n\nUse the GitHub API to query deployment history:\n\n```bash\ngh api /repos/{owner}/{repo}/deployments \\\n  --jq '.[] | {sha: .sha, created_at: .created_at, creator: .creator.login}'\n```",
    "why": "Centralised deployment history in GitHub means any engineer can answer 'what is deployed right now' in seconds, without access to your CI system or cloud console.",
    "gotchas": [
      "Deployment records are created at job start, not step start—the environment URL is shown even during deployment",
      "Inactive environments are not automatically cleaned up; old environment names accumulate in the UI",
      "GitHub Deployments API can be used by external tools (Datadog, Grafana) to annotate dashboards with deploy events"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["environments", "deployment history", "audit trail", "deployments API", "who deployed", "when deployed"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": null
  },
  {
    "title": "OIDC authentication in CI eliminates long-lived cloud credentials",
    "category": "principle",
    "tags": ["github-actions", "security", "OIDC", "aws", "gcp"],
    "problem": "CI workflows store long-lived cloud credentials (AWS access keys, GCP service account keys) as GitHub secrets. These keys do not expire, are copied into secrets managers, and rotate infrequently. A leaked key grants persistent access.",
    "solution": "Use OpenID Connect (OIDC) to get short-lived credentials directly from the cloud provider:\n\n```yaml\n# AWS example\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n      - uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::123456789:role/GitHubActionsRole\n          aws-region: us-east-1\n\n      - run: aws s3 ls  # no credentials stored anywhere\n```\n\nIAM role trust policy (allows only this repo's main branch):\n\n```json\n{\n  \"Condition\": {\n    \"StringEquals\": {\n      \"token.actions.githubusercontent.com:sub\": \"repo:org/repo:ref:refs/heads/main\",\n      \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n    }\n  }\n}\n```",
    "why": "OIDC tokens expire after the job ends (typically 1 hour). There is nothing to rotate, nothing to store, and no credential that outlives the job. The trust policy restricts which repos and branches can assume the role.",
    "gotchas": [
      "permissions: id-token: write must be explicitly declared—it is not on by default",
      "The sub claim format varies by trigger type (branch vs PR vs tag)—test the exact claim value in IAM before going to production",
      "GCP uses Workload Identity Federation; AWS uses STS AssumeRoleWithWebIdentity—both require one-time setup of the trust relationship"
    ],
    "language": "bash",
    "framework": null,
    "environment": ["aws", "gcp"],
    "error_messages": [
      "Error: Credentials could not be loaded",
      "Not authorized to perform sts:AssumeRoleWithWebIdentity"
    ],
    "keywords": ["OIDC", "short-lived credentials", "IAM role", "workload identity", "no long-lived secrets", "STS"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": "aws-actions/configure-aws-credentials@v4"
  },
  {
    "title": "Changelog generation with git-cliff from conventional commits",
    "category": "pattern",
    "tags": ["changelog", "git-cliff", "conventional-commits", "release"],
    "problem": "Writing CHANGELOGs manually is tedious and inevitably incomplete. Auto-generated changelogs from all commits are noisy and include irrelevant chore and CI commits that users do not care about.",
    "solution": "Use git-cliff to generate filtered, categorised changelogs:\n\n```yaml\n# cliff.toml\n[changelog]\nheader = \"# Changelog\\n\\n\"\nbody = \"\"\"\n{% for group, commits in commits | group_by(attribute=\"group\") %}\n### {{ group | striptags | trim | upper_first }}\n{% for commit in commits %}\n- {{ commit.message | upper_first }}{% if commit.breaking %} [**breaking**]{% endif %}\n{% endfor %}\n{% endfor %}\n\"\"\"\ntrim = true\n\n[git]\nconventional_commits = true\nfilter_unconventional = true\ncommit_parsers = [\n  { message = \"^feat\", group = \"Features\" },\n  { message = \"^fix\", group = \"Bug Fixes\" },\n  { message = \"^chore|^ci|^build\", skip = true },\n]\n```\n\nIn CI:\n\n```yaml\n- uses: orhun/git-cliff-action@v3\n  with:\n    config: cliff.toml\n    args: --latest --strip header\n  env:\n    OUTPUT: CHANGELOG.md\n```",
    "why": "skip = true for chore/ci/build commits removes noise. Grouping by type gives users a clear view of what changed. --latest generates only the changelog for the current release, not the entire history.",
    "gotchas": [
      "git-cliff requires full git history—use fetch-depth: 0 in the checkout step",
      "Breaking changes are only detected if the commit uses the ! suffix or includes 'BREAKING CHANGE:' in the footer",
      "The generated CHANGELOG.md must be committed back to the repo by the release workflow"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [],
    "keywords": ["git-cliff", "changelog", "conventional commits", "release notes", "filter", "breaking change"],
    "severity": "tip",
    "context": null,
    "code_snippets": [],
    "version_info": "orhun/git-cliff-action@v3"
  },
  {
    "title": "GitHub Actions workflow permissions: principle of least privilege",
    "category": "principle",
    "tags": ["github-actions", "security", "permissions", "GITHUB_TOKEN"],
    "problem": "By default, the GITHUB_TOKEN in GitHub Actions has read/write permissions to all repository resources. A compromised action or a supply chain attack can use this token to push code, modify releases, or exfiltrate data.",
    "solution": "Set minimum permissions at the workflow level and override per-job as needed:\n\n```yaml\n# Workflow-level default: read-only\npermissions:\n  contents: read\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    # inherits read-only from workflow level\n    steps:\n      - uses: actions/checkout@v4\n      - run: npm test\n\n  release:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write       # to create GitHub releases\n      packages: write       # to push to GHCR\n      pull-requests: write  # to comment on PRs\n    steps:\n      - run: ./release.sh\n```\n\nAudit which permissions actions require by reading their action.yml or documentation before adding them.",
    "why": "Least privilege limits blast radius. A compromised action in the test job cannot push to main or create releases if it only has contents: read. Job-level overrides are more granular than workflow-level permissions.",
    "gotchas": [
      "Setting permissions at the workflow level removes all default permissions—you must explicitly grant everything the job needs",
      "GITHUB_TOKEN permissions cannot exceed the repository settings—organisation-level restrictions apply",
      "Third-party actions that use GITHUB_TOKEN inherit the calling job's permissions—review what each action needs"
    ],
    "language": "bash",
    "framework": null,
    "environment": [],
    "error_messages": [
      "Error: Resource not accessible by integration",
      "HttpError: Resource not accessible by integration"
    ],
    "keywords": ["permissions", "GITHUB_TOKEN", "least privilege", "security", "contents: read", "supply chain"],
    "severity": "major",
    "context": null,
    "code_snippets": [],
    "version_info": null
  }
]
